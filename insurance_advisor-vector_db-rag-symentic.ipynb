{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaur\\AppData\\Roaming\\Python\\Python39\\site-packages\\langchain_community\\document_loaders\\blob_loaders\\file_system.py:5: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_community.document_loaders.blob_loaders.schema import Blob, BlobLoader\n",
      "C:\\Users\\shaur\\AppData\\Roaming\\Python\\Python39\\site-packages\\langchain_community\\document_loaders\\__init__.py:221: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_community.document_loaders.youtube import (\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\shaur\\\\Desktop\\\\icici policies wording'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, nltk, re\n",
    "from langchain.document_loaders import PyPDFLoader, WebBaseLoader\n",
    "from langchain.text_splitter import NLTKTextSplitter,RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from pathlib import Path as p\n",
    "os.environ[\"OPENAI_API_KEY\"]=open(r\"C:\\Users\\shaur\\Downloads\\chat_gpt_key.txt\",'r').read()\n",
    "os.chdir(r\"C:\\Users\\shaur\\Desktop\\icici policies wording\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of pages from Comparison.pdf:- 3\n",
      "Length of pages from Elevate Policy Wordings.pdf:- 45\n",
      "Length of pages from Health AdvantEdge_Policy wordings_IL.pdf:- 51\n",
      "Length of pages from policy-wordings_maxprotect.pdf:- 40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['Comparison.pdf', 'Elevate Policy Wordings.pdf', 'Health AdvantEdge_Policy wordings_IL.pdf', 'policy-wordings_maxprotect.pdf'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_policies_pdfs = [i for i in os.listdir() if '.pdf' in i]\n",
    "all_policies_dict = {}\n",
    "for i in range(0,len(all_policies_pdfs)):\n",
    "    loader = PyPDFLoader(all_policies_pdfs[i])\n",
    "    pages = loader.load()\n",
    "    pages = [page.page_content for page in pages]\n",
    "    print(f\"Length of pages from {all_policies_pdfs[i]}:- {len(pages)}\")\n",
    "    all_policies_dict[all_policies_pdfs[i]] = pages\n",
    "all_policies_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### (FAILED) getting summary from Policybazar for the comparision of these 3 policy\n",
    "# loader = WebBaseLoader(\"https://pbhealth.policybazaar.com/quote-compare-2?encenq=emRrNkN3aEk4TlBwdzZwLzc4NXRZMGRlRzFkbE5pL2JYQmlOaFBLUm5iUT0&enquiryid=NzU3NzI1ODk0&k=eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJFbnF1aXJ5SWQiOjc1NzcyNTg5NCwiRXhwaXJ5VGltZSI6MTc0NTc0MjM2OSwiUm9sZSI6bnVsbH0.GkSkfmCCjxhe3pYejDkhMyVvKZj2LxVR3JDBOujUE1RzG3W-oWkxJ32Nyb9e33WEvEVqflbUduzokJ5Nju8XepIY0kjivduTKeyfZhljIMkWjCZ9IxJnYw9_dQg23z0BAldA36UL9Tw-e6VgM46Do7ehY7byPWdYfRcE1zCkQyc&plan=80950-1000000-Elevate-3-ICICI%20Lombard-8739-0-2-1&plan=80843-10000000-Max%20Protect%20Classic-3-ICICI%20Lombard-16724-0-2-1&plan=80045-1000000-Health%20AdvantEdge-3-ICICI%20Lombard-12282-0-2-1&profileid=147272833&utm_source=MYACC_M\")\n",
    "# documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_policies_dict['Elevate Policy Wordings.pdf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_policies_dict['Health AdvantEdge_Policy wordings_IL.pdf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_policies_dict['policy-wordings_maxprotect.pdf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Removing footer details from the PDF's\n",
    "\n",
    "repeted_text_0 = 'IRDA Reg. No. 115\\nMailing Address:\\n601 / 602, 6th Floor, Interface Building No. 16,\\nNew Link Road, Malad (West), \\nMumbai - 400 064.CIN:  L67200MH2000PLC129408\\nRegistered Office Address:\\nICICI Lombard House, 414, P Balu Marg, Off \\nVeer Savarkar Road, Nr Siddhi Vinayak Temple, \\nPrabhadevi, Mumbai - 400 025.UIN: ICIHLIP25048V042425 Product Name: Elevate\\nToll free No.  : 1800 2666  \\nAlternate No.: 86552 22666 (Chargeable)\\nWebsite :  www.iciclombard.com\\nE-mail : customersupport@icicilombard.comICICI Lombard General Insurance Company Limited'\n",
    "repeted_text_1 = 'Health AdvantEdge  \\nICICI Lombard General Insurance Company Limited  \\n       IRDA  Reg. No. 115                            CIN : L67200MH2000PLC129408                                  UIN: ICIHLIP24182V042324     Health A dvantEdge  \\n       Mailing A ddress:                              Registered Office A ddress:                                Toll free no : 1800 2666   \\n601 & 602, 6th Floor, Interf ace 16,    ICICI Lombard House, 414, P Balu Marg,                     A lternate no :  86552 2 2666 (chargeable)   \\nNew Linking Road, Malad (West)      Of f  Veer Sav arkar Road, Nr Siddhi Vinay ak Temple,    E-mail : customersupport@icicilombard.com  \\n    Mumbai - 400 064                             Prabhadev i, Mumbai 400 025                                        Website : www.icicilombard.com   '\n",
    "repeted_text_2 = 'Annexure I  MaxProtect  Policy w ordings  \\nIRDAI reg. no.: 115                CIN: L67200MH20000PLC129408   UIN: ICIHLIP24084V012324'\n",
    "for i in all_policies_dict.keys():\n",
    "    for k in [repeted_text_0,repeted_text_1,repeted_text_2]:\n",
    "        all_policies_dict[i] = [page.replace(k.strip(), '').strip() for page in all_policies_dict[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Symentic based chunking\n",
    "### Using RecursiveCharacterTextSplitter (found that the context was missing or changed and the sentenses were getting cut from between)\n",
    "### Using NLTKTextSplitter (found that the context was being saved and the sentenses were complete)\n",
    "# splitter = NLTKTextSplitter(chunk_size=1200, chunk_overlap=300)\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1200,\n",
    "    chunk_overlap=400,\n",
    "    separators=[\"\\n\\n\", \"\\n\", r\"\\. \\s\"]\n",
    ")\n",
    "Comparison = all_policies_dict['Comparison.pdf']\n",
    "all_policies_dict.pop('Comparison.pdf')\n",
    "chunked_policies = {}\n",
    "for i, full_text in all_policies_dict.items():\n",
    "    full_text = \"\\n\".join(full_text)\n",
    "    docs = splitter.create_documents([full_text])\n",
    "    chunked_policies[i] = docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a. PREAMBLE:\n",
      "  ICICI Lombard General Insurance Company Limited \n",
      "(“We/Us”), having received a Proposal and the premium \n",
      "from the Proposer named in Part A of the Policy \n",
      "(hereinafter referred to as the “Policy Schedule”) and the \n",
      "said Proposal form with any statement, report or other \n",
      "document leading to the issue of this Policy and referred \n",
      "to therein having been accepted and agreed to by Us \n",
      "and the Proposer as the basis of this contract do, by this \n",
      "Policy agree, in consideration of and subject to the due \n",
      "receipt of the subsequent premiums, as set out in the \n",
      "Policy Schedule. \n",
      "  Further, subject to the Policy terms and conditions that \n",
      "on proof to Our satisfaction of the compensation having \n",
      "become payable as set out in the Policy Schedule to the \n",
      "said person or persons claiming payment or occurencean \n",
      "event upon which one or more benefits become payable \n",
      "under this Policy, the Annual Sum Insured / appropriate \n",
      "benefit amount will be paid by Us.\n",
      "b. DEFINITIONS:\n",
      "  For the purposes of this Policy, the terms specified below \n",
      "shall have the meaning set forth wherever appearing/\n",
      "specified in this Policy or related Add-ons/Optional \n",
      "Covers:\n"
     ]
    }
   ],
   "source": [
    "print(chunked_policies['Elevate Policy Wordings.pdf'][0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Elevate Policy Wordings.pdf', 'Health AdvantEdge_Policy wordings_IL.pdf', 'policy-wordings_maxprotect.pdf'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_policies.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Elevate Policy Wordings.pdf no. of chuncks:- 253\n",
      "For Health AdvantEdge_Policy wordings_IL.pdf no. of chuncks:- 221\n",
      "For policy-wordings_maxprotect.pdf no. of chuncks:- 138\n"
     ]
    }
   ],
   "source": [
    "for i in chunked_policies.keys():\n",
    "    print(f\"For {i} no. of chuncks:- {len(chunked_policies[i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Elevate Policy Wordings.pdf Average words in each chunck:- 1170.5335968379447\n",
      "For Health AdvantEdge_Policy wordings_IL.pdf Average words in each chunck:- 1144.9864253393664\n",
      "For policy-wordings_maxprotect.pdf Average words in each chunck:- 1159.6159420289855\n"
     ]
    }
   ],
   "source": [
    "for i in chunked_policies.keys():\n",
    "    len_temp = 0\n",
    "    for k in range(0,len(chunked_policies[i])):\n",
    "        len_temp += len(chunked_policies[i][k].page_content)\n",
    "    print(f\"For {i} Average words in each chunck:- {len_temp/len(chunked_policies[i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens to be used:- 165831\n"
     ]
    }
   ],
   "source": [
    "### Making an instance just to count numbers of tokens and use summarize to make summary of all policies\n",
    "### Temperature 0 is for the reason that we do not want it to be creative\n",
    "llm = ChatOpenAI(temperature=0,model=\"gpt-3.5-turbo\")\n",
    "total=0\n",
    "for i in chunked_policies.keys():\n",
    "    for k in range(0,len(chunked_policies[i])):\n",
    "        total+=llm.get_num_tokens(chunked_policies[i][k].page_content)\n",
    "print(f\"Total Tokens to be used:- {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FAILED LOGIC (Tried to summarize the 3 docs to reduce the token size)\n",
    "### the chain type have 3 type \n",
    "### map_reduce (each chuck is summarised and Combines all partial summaries into one final summary), \n",
    "### refine (keeps on refining when new chunck is added, much detailed), \n",
    "### stuff (load all chucks at onces and used as single prompt to summarize)\n",
    "\n",
    "# summary_chain = load_summarize_chain(llm, chain_type=\"refine\")\n",
    "# final_results = {}\n",
    "# for policy_name, docs in chunked_policies.items():\n",
    "#     print(f\"Summarizing policy: {policy_name}\")\n",
    "#     try:\n",
    "#         map_reduce_outputs = summary_chain({\"input_documents\": docs})\n",
    "#         final_summary = map_reduce_outputs[\"output_text\"]\n",
    "        \n",
    "#         summary_dir = \"policy_summaries\"\n",
    "#         os.makedirs(summary_dir, exist_ok=True)\n",
    "#         clean_filename = policy_name.replace('.pdf', '').replace(' ', '_')\n",
    "#         output_path = os.path.join(summary_dir, f\"{clean_filename}_summary.txt\")\n",
    "#         with open(output_path, 'w', encoding='utf-8') as f:\n",
    "#             f.write(final_summary)\n",
    "        \n",
    "#         print(f\"Summary for {policy_name} saved to {output_path}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing {policy_name}: {e}\")\n",
    "        \n",
    "# print(\"All policy summaries completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Other Logic to use Vector DB to make a local DB storage and use emmbeding of all the chuncks\n",
    "### creating embedding for all documents that has to be stored in vector db\n",
    "### Embeddings are made because ChromaDB stores and retrieves data based on semantic similarity, and embeddings are the numerical representations that capture this semantic meaning.\n",
    "embeddings = OpenAIEmbeddings()\n",
    "all_docs_list = []\n",
    "for policy_name, docs in chunked_policies.items():\n",
    "    for doc in docs:\n",
    "        doc.metadata['source_policy'] = policy_name\n",
    "        all_docs_list.append(doc)\n",
    "#Creating local vector db\n",
    "persist_directory = 'db_chroma'\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=all_docs_list,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________________________________________________________________________________________\n",
      "User:- \n",
      " Hi\n",
      "Assistant:- \n",
      " Hello! How can I assist you today?\n",
      "_________________________________________________________________________________________________________________________________________________\n",
      "User:- \n",
      " mujko apni wife k liye insurance lena h\n",
      "Assistant:- \n",
      " Kya aap Elevate Policy Wordings, Health AdvantEdge_Policy wordings_IL, ya Maxprotect policy ke liye insurance lena chahte hain?\n",
      "_________________________________________________________________________________________________________________________________________________\n",
      "User:- \n",
      " we are looking for a cover upto 20 lacks, my wife recently had her gal bladder removed and we are planning for a baby soon\n",
      "Assistant:- \n",
      " Apologies for the delay in response. \n",
      " Based on your needs for maternity coverage and higher sum insured, I recommend the Max Protect Classic policy with a cover of ₹1 Cr. \n",
      " This policy includes coverage for maternity expenses, including complications arising from pregnancy. \n",
      " It also offers post-hospitalization benefits up to 180 days, which can be beneficial for your wife post her gallbladder surgery. \n",
      "\n",
      "_________________________________________________________________________________________________________________________________________________\n",
      "User:- \n",
      " other policy does not have maternity benifits?\n",
      "Assistant:- \n",
      " Yes, the Elevate Policy Wordings and Health AdvantEdge_Policy wordings_IL do not offer maternity benefits. \n",
      "\n",
      "_________________________________________________________________________________________________________________________________________________\n",
      "User:- \n",
      " can you explain a bit more about the benifits of Max Protect Classic considering pregnancy?\n",
      "Assistant:- \n",
      " Max Protect Classic offers Maternity Benefit_A and Maternity Benefit_B options for pregnancy-related expenses. \n",
      " Maternity Benefit_A covers up to 10% of the Annual Sum Insured, with a maximum limit of INR 1 Lakh for delivery or lawful termination of pregnancy. \n",
      " Maternity Benefit_B covers up to 10% of the Annual Sum Insured, with a maximum limit of INR 10 Lakhs for delivery or termination due to life-threatening situations. \n",
      " Both options have waiting periods before coverage starts. \n",
      " Maternity Benefit_B also has a waiting period of 9 months. \n",
      " These benefits are available for insured females aged 18 to 50 years. \n",
      "\n",
      "_________________________________________________________________________________________________________________________________________________\n",
      "User:- \n",
      " okay, thank you. i shall proceed with the buying process.\n",
      "Assistant:- \n",
      " Great! If you need any further assistance or have any more questions during the buying process, feel free to reach out. \n",
      " I'm here to help you make an informed decision. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(temperature=1,model=\"gpt-3.5-turbo\")\n",
    "### Setting up Prompt\n",
    "prompt_template = \"\"\"You are a professional insurance advisor specializing in Elevate Policy Wordings, Health AdvantEdge_Policy wordings_IL, and Maxprotect. \n",
    "You must use the provided context and chat history to answer the user's questions accurately and comprehensively.\n",
    "If the user's query lacks sufficient information, ask clarifying questions to obtain these details. \n",
    "Be proactive in gathering information to provide personalized advice.\n",
    "Explain policy features in the context of the user's needs. For example, if discussing maternity coverage, explain how it would benefit a user planning to start a family.\n",
    "here you can find the major diffrance in all insurance offered {comparision}\n",
    "\n",
    "If the user asks about pricing, refer them to Shaurya Vashisth.\n",
    "\n",
    "The communication can be in english, hindi or hinglish.\n",
    "Give answers in small sentenses.\n",
    "\n",
    "User History: {user_history}\n",
    "Assistant History: {assistant_history}\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"comparision\",\"user_history\",\"assistant_history\",\"context\", \"question\"]\n",
    ")\n",
    "### Setting up RetrievalQA to retive similar chuncks\n",
    "### K = number of top k relevent chuncks\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "\n",
    "chat_logs = {\"User\":\"\",\"Assistant\":\"\"}\n",
    "\n",
    "while True:\n",
    "    Query = input(\"User:- \")\n",
    "    if Query.lower() == \"quit\":\n",
    "        break\n",
    "    else:\n",
    "        print(\"_________________________________________________________________________________________________________________________________________________\")\n",
    "        docs = retriever.invoke(Query)\n",
    "        full_prompt = PROMPT.format(user_history=chat_logs['User'],\n",
    "                assistant_history=chat_logs['Assistant'],\n",
    "                context=\"\\n\\n\".join(doc.page_content for doc in docs),\n",
    "                question=Query,\n",
    "                comparision = Comparison)\n",
    "        response = llm.invoke(full_prompt)\n",
    "        print(f\"User:- \\n {Query}\")\n",
    "        print(\"Assistant:- \\n \" + response.content.replace('.', '. \\n'))\n",
    "        chat_logs['User'] = Query\n",
    "        chat_logs['Assistant'] = response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### build a logic to summarize the chats older that 5 iter\n",
    "#### Tune on temprature, top-p and top-k\n",
    "\n",
    "\n",
    "# The final step is to generate the next token by sampling from this distribution \n",
    "# The temperature hyperparameter plays a critical role in this process. Mathematically speaking,\n",
    "\n",
    "# types of retriever Maximun Marginal Relevance (MMR), Multi query  retrival (MQR), Contextual Compression Retriver (CCR)\n",
    "# it is a very simple operation: model output logits are simply divided by the temperature:\n",
    "# temperature = 1: Dividing logits by one has no effect on the softmax outputs.\n",
    "# temperature < 1: Lower temperature makes the model more confident and deterministic by sharpening the probability distribution, leading to more predictable outputs.\n",
    "# temperature > 1: Higher temperature creates a softer probability distribution, allowing for more randomness in the generated text – what some refer to as model “creativity”.\n",
    "\n",
    "# In addition, the sampling process can be further refined using top-k and top-p parameters:\n",
    "# top-k sampling: Limits the candidate tokens to the top k tokens with the highest probabilities, filtering out less likely options.\n",
    "# top-p sampling: Considers the smallest set of tokens whose cumulative probability exceeds a threshold p, ensuring that only the most likely tokens contribute while still allowing for diversity.\n",
    "# By tuning temperature, top-k, and top-p, you can balance between deterministic and diverse outputs, tailoring the model's behavior to your specific needs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
