{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaur\\AppData\\Roaming\\Python\\Python39\\site-packages\\langchain_community\\document_loaders\\blob_loaders\\file_system.py:5: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_community.document_loaders.blob_loaders.schema import Blob, BlobLoader\n",
      "C:\\Users\\shaur\\AppData\\Roaming\\Python\\Python39\\site-packages\\langchain_community\\document_loaders\\__init__.py:221: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_community.document_loaders.youtube import (\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\shaur\\\\Desktop\\\\icici policies wording'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, nltk, re\n",
    "from langchain.document_loaders import PyPDFLoader, WebBaseLoader\n",
    "from langchain.text_splitter import NLTKTextSplitter,RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from pathlib import Path as p\n",
    "os.environ[\"OPENAI_API_KEY\"]=open(r\"C:\\Users\\shaur\\Downloads\\chat_gpt_key.txt\",'r').read()\n",
    "os.chdir(r\"C:\\Users\\shaur\\Desktop\\icici policies wording\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of pages from Comparison.pdf:- 3\n",
      "Length of pages from Elevate Policy Wordings.pdf:- 45\n",
      "Length of pages from Health AdvantEdge_Policy wordings_IL.pdf:- 51\n",
      "Length of pages from policy-wordings_maxprotect.pdf:- 40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['Comparison.pdf', 'Elevate Policy Wordings.pdf', 'Health AdvantEdge_Policy wordings_IL.pdf', 'policy-wordings_maxprotect.pdf'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_policies_pdfs = [i for i in os.listdir() if '.pdf' in i]\n",
    "all_policies_dict = {}\n",
    "for i in range(0,len(all_policies_pdfs)):\n",
    "    loader = PyPDFLoader(all_policies_pdfs[i])\n",
    "    pages = loader.load()\n",
    "    pages = [page.page_content for page in pages]\n",
    "    print(f\"Length of pages from {all_policies_pdfs[i]}:- {len(pages)}\")\n",
    "    all_policies_dict[all_policies_pdfs[i]] = pages\n",
    "all_policies_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### (FAILED) getting summary from Policybazar for the comparision of these 3 policy\n",
    "# loader = WebBaseLoader(\"https://pbhealth.policybazaar.com/quote-compare-2?encenq=emRrNkN3aEk4TlBwdzZwLzc4NXRZMGRlRzFkbE5pL2JYQmlOaFBLUm5iUT0&enquiryid=NzU3NzI1ODk0&k=eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJFbnF1aXJ5SWQiOjc1NzcyNTg5NCwiRXhwaXJ5VGltZSI6MTc0NTc0MjM2OSwiUm9sZSI6bnVsbH0.GkSkfmCCjxhe3pYejDkhMyVvKZj2LxVR3JDBOujUE1RzG3W-oWkxJ32Nyb9e33WEvEVqflbUduzokJ5Nju8XepIY0kjivduTKeyfZhljIMkWjCZ9IxJnYw9_dQg23z0BAldA36UL9Tw-e6VgM46Do7ehY7byPWdYfRcE1zCkQyc&plan=80950-1000000-Elevate-3-ICICI%20Lombard-8739-0-2-1&plan=80843-10000000-Max%20Protect%20Classic-3-ICICI%20Lombard-16724-0-2-1&plan=80045-1000000-Health%20AdvantEdge-3-ICICI%20Lombard-12282-0-2-1&profileid=147272833&utm_source=MYACC_M\")\n",
    "# documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_policies_dict['Elevate Policy Wordings.pdf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_policies_dict['Health AdvantEdge_Policy wordings_IL.pdf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_policies_dict['policy-wordings_maxprotect.pdf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Removing footer details from the PDF's\n",
    "\n",
    "repeted_text_0 = 'IRDA Reg. No. 115\\nMailing Address:\\n601 / 602, 6th Floor, Interface Building No. 16,\\nNew Link Road, Malad (West), \\nMumbai - 400 064.CIN:  L67200MH2000PLC129408\\nRegistered Office Address:\\nICICI Lombard House, 414, P Balu Marg, Off \\nVeer Savarkar Road, Nr Siddhi Vinayak Temple, \\nPrabhadevi, Mumbai - 400 025.UIN: ICIHLIP25048V042425 Product Name: Elevate\\nToll free No.  : 1800 2666  \\nAlternate No.: 86552 22666 (Chargeable)\\nWebsite :  www.iciclombard.com\\nE-mail : customersupport@icicilombard.comICICI Lombard General Insurance Company Limited'\n",
    "repeted_text_1 = 'Health AdvantEdge  \\nICICI Lombard General Insurance Company Limited  \\n       IRDA  Reg. No. 115                            CIN : L67200MH2000PLC129408                                  UIN: ICIHLIP24182V042324     Health A dvantEdge  \\n       Mailing A ddress:                              Registered Office A ddress:                                Toll free no : 1800 2666   \\n601 & 602, 6th Floor, Interf ace 16,    ICICI Lombard House, 414, P Balu Marg,                     A lternate no :  86552 2 2666 (chargeable)   \\nNew Linking Road, Malad (West)      Of f  Veer Sav arkar Road, Nr Siddhi Vinay ak Temple,    E-mail : customersupport@icicilombard.com  \\n    Mumbai - 400 064                             Prabhadev i, Mumbai 400 025                                        Website : www.icicilombard.com   '\n",
    "repeted_text_2 = 'Annexure I  MaxProtect  Policy w ordings  \\nIRDAI reg. no.: 115                CIN: L67200MH20000PLC129408   UIN: ICIHLIP24084V012324'\n",
    "for i in all_policies_dict.keys():\n",
    "    for k in [repeted_text_0,repeted_text_1,repeted_text_2]:\n",
    "        all_policies_dict[i] = [page.replace(k.strip(), '').strip() for page in all_policies_dict[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Symentic based chunking\n",
    "### Using RecursiveCharacterTextSplitter (found that the context was missing or changed and the sentenses were getting cut from between)\n",
    "### Using NLTKTextSplitter (found that the context was being saved and the sentenses were complete)\n",
    "# splitter = NLTKTextSplitter(chunk_size=1200, chunk_overlap=300)\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2500,\n",
    "    chunk_overlap=1000,\n",
    "    separators=[\"\\n\\n\", \"\\n\", r\"\\. \\s\"]\n",
    ")\n",
    "Comparison = all_policies_dict['Comparison.pdf']\n",
    "all_policies_dict.pop('Comparison.pdf')\n",
    "chunked_policies = {}\n",
    "for i, full_text in all_policies_dict.items():\n",
    "    full_text = \"\\n\".join(full_text)\n",
    "    docs = splitter.create_documents([full_text])\n",
    "    chunked_policies[i] = docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a. PREAMBLE:\n",
      "  ICICI Lombard General Insurance Company Limited \n",
      "(“We/Us”), having received a Proposal and the premium \n",
      "from the Proposer named in Part A of the Policy \n",
      "(hereinafter referred to as the “Policy Schedule”) and the \n",
      "said Proposal form with any statement, report or other \n",
      "document leading to the issue of this Policy and referred \n",
      "to therein having been accepted and agreed to by Us \n",
      "and the Proposer as the basis of this contract do, by this \n",
      "Policy agree, in consideration of and subject to the due \n",
      "receipt of the subsequent premiums, as set out in the \n",
      "Policy Schedule. \n",
      "  Further, subject to the Policy terms and conditions that \n",
      "on proof to Our satisfaction of the compensation having \n",
      "become payable as set out in the Policy Schedule to the \n",
      "said person or persons claiming payment or occurencean \n",
      "event upon which one or more benefits become payable \n",
      "under this Policy, the Annual Sum Insured / appropriate \n",
      "benefit amount will be paid by Us.\n",
      "b. DEFINITIONS:\n",
      "  For the purposes of this Policy, the terms specified below \n",
      "shall have the meaning set forth wherever appearing/\n",
      "specified in this Policy or related Add-ons/Optional \n",
      "Covers:\n",
      "  Where the context so requires, references to the singular \n",
      "shall also include references to the plural and references \n",
      "to any gender shall include references to all genders. \n",
      "Further any references to statutory enactment include \n",
      "subsequent changes to the same.\n",
      "i.\t Standard\tDefinitions\t\n",
      "  “Accident”  means a sudden, unforeseen and involuntary \n",
      "event caused by external, visible and violent means. \n",
      " \t“Any\tone\tIllness”  means continuous period of Illness \n",
      "and it includes a relapse within 45 days from the date of \n",
      "last consultation with the Hospital/Nursing Home where \n",
      "treatment may have been taken. \n",
      "  “Ayush\tTreatment”  refers to the medical and / or \n",
      "hospitalization treatments given under ‘Ayurveda, Yoga \n",
      "and Naturopathy, Unani, Siddha and Homeopathy \n",
      "systems. \n",
      " \t“Break\tin\tpolicy”  means the period of gap that occurs at \n",
      "the end of the existing policy term / installment premium \n",
      "due date, when the premium due for renewal on a given \n",
      "policy or installment premium due is not paid on or before \n",
      "the premium renewal date or grace period. \n",
      "  “Cashless\t facility”  means a facility extended by the \n",
      "Insurer to the Insured where, the payments of the costs \n",
      "of treatment undergone by the Insured in accordance \n",
      "with the Policy terms and conditions are directly made \n",
      "to the network provider by the Insurer to the extent pre-\n"
     ]
    }
   ],
   "source": [
    "print(chunked_policies['Elevate Policy Wordings.pdf'][0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Elevate Policy Wordings.pdf', 'Health AdvantEdge_Policy wordings_IL.pdf', 'policy-wordings_maxprotect.pdf'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_policies.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elevate Policy Wordings.pdf:- 134\n",
      "Health AdvantEdge_Policy wordings_IL.pdf:- 117\n",
      "policy-wordings_maxprotect.pdf:- 73\n"
     ]
    }
   ],
   "source": [
    "for i in chunked_policies.keys():\n",
    "    print(f\"{i}:- {len(chunked_policies[i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens to be used:- 186080\n"
     ]
    }
   ],
   "source": [
    "### Making an instance just to count numbers of tokens and use summarize to make summary of all policies\n",
    "### Temperature 0 is for the reason that we do not want it to be creative\n",
    "llm = ChatOpenAI(temperature=0,model=\"gpt-3.5-turbo\")\n",
    "total=0\n",
    "for i in chunked_policies.keys():\n",
    "    for k in range(0,len(chunked_policies[i])):\n",
    "        total+=llm.get_num_tokens(chunked_policies[i][k].page_content)\n",
    "print(f\"Total Tokens to be used:- {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FAILED LOGIC (Tried to summarize the 3 docs to reduce the token size)\n",
    "### the chain type have 3 type \n",
    "### map_reduce (each chuck is summarised and Combines all partial summaries into one final summary), \n",
    "### refine (keeps on refining when new chunck is added, much detailed), \n",
    "### stuff (load all chucks at onces and used as single prompt to summarize)\n",
    "\n",
    "# summary_chain = load_summarize_chain(llm, chain_type=\"refine\")\n",
    "# final_results = {}\n",
    "# for policy_name, docs in chunked_policies.items():\n",
    "#     print(f\"Summarizing policy: {policy_name}\")\n",
    "#     try:\n",
    "#         map_reduce_outputs = summary_chain({\"input_documents\": docs})\n",
    "#         final_summary = map_reduce_outputs[\"output_text\"]\n",
    "        \n",
    "#         summary_dir = \"policy_summaries\"\n",
    "#         os.makedirs(summary_dir, exist_ok=True)\n",
    "#         clean_filename = policy_name.replace('.pdf', '').replace(' ', '_')\n",
    "#         output_path = os.path.join(summary_dir, f\"{clean_filename}_summary.txt\")\n",
    "#         with open(output_path, 'w', encoding='utf-8') as f:\n",
    "#             f.write(final_summary)\n",
    "        \n",
    "#         print(f\"Summary for {policy_name} saved to {output_path}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing {policy_name}: {e}\")\n",
    "        \n",
    "# print(\"All policy summaries completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Other Logic to use Vector DB to make a local DB storage and use emmbeding of all the chuncks\n",
    "### creating embedding for all documents that has to be stored in vector db\n",
    "### Embeddings are made because ChromaDB stores and retrieves data based on semantic similarity, and embeddings are the numerical representations that capture this semantic meaning.\n",
    "embeddings = OpenAIEmbeddings()\n",
    "all_docs_list = []\n",
    "for policy_name, docs in chunked_policies.items():\n",
    "    for doc in docs:\n",
    "        doc.metadata['source_policy'] = policy_name\n",
    "        all_docs_list.append(doc)\n",
    "#Creating local vector db\n",
    "persist_directory = 'db_chroma'\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=all_docs_list,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:-  Hi\n",
      "Assistant:-  Hello! How can I help you today?\n",
      "User:-  mujko apni wife k liye insurance lena h\n",
      "Assistant:-  Sure, apni wife ke liye insurance lena chahte hain. Kya aap mujhe bata sakte hain ki aapki wife ki age kya hai aur aap kis type ka coverage dekh rahe hain?\n",
      "User:-  my wife age is 32 and we recently had a gal bladder removal surgeries and we are planning to have a baby soon\n",
      "Assistant:-  Based on your wife's age of 32, you may consider the Elevate (₹10 Lakh Cover) policy for comprehensive coverage that includes room rent limit, restoration of cover, and renewal bonus benefits. Additionally, it covers specific illnesses after 24 months and pre-hospitalization expenses up to ₹10 lakh for 90 days.\n",
      "For pregnancy-related coverage, the Health AdvantEdge policy could be suitable, providing coverage for complications arising from pregnancy up to ₹5 lakhs. However, please note exclusions related to delivery expenses.\n",
      "Before planning a baby, ensure the policy's waiting periods and coverage are aligned with your needs, especially post-delivery benefits like those under the maternity coverage section. Consider your recent gall bladder surgery and any specific medical history when choosing the right policy for your family's future needs.\n",
      "User:-  which Policy i should consider as we are looking forward to welcome our baby\n",
      "Assistant:-  Based on your plan to welcome a baby and the recent gall bladder surgery, you should consider the Health AdvantEdge policy for cover during pregnancy and maternity-related expenses. This policy also includes coverage for newborn baby expenses, vaccinations, and critical illness benefits.\n",
      "User:-  okay, thanks can you process my request for Health AdvantEdge policy, i am looking forward to buy it\n",
      "Assistant:-  Sure, I can assist with that. Can you please confirm the sum insured amount and any specific coverage you are looking for in the Health AdvantEdge policy?\n",
      "User:-  20 lacks will be the coverage\n",
      "Assistant:-  \n",
      "The Health AdvantEdge policy offers a coverage of ₹10 lakh, not ₹20 lakh.\n",
      "User:-  where is it written?\n",
      "Assistant:-  It is written in the terms and conditions of the policy under the section \"Territorial Limit\".\n",
      "User:-  can i have other policy were i can get upto 20 lacks coverage and additional riders for pregnancy coverage\n",
      "Assistant:-  To explore a policy with up to 20 lakhs coverage and additional riders for pregnancy coverage, you can consider the Max Protect Classic plan. It offers a coverage of up to 1 crore with optional maternity benefits. This policy can be a suitable option for your requirements.\n",
      "User:-  okay i will go for Max Protect Classic and maternity benefits\n",
      "Assistant:-  For your requirement of up to 20 lakh coverage with additional riders for pregnancy, the Max Protect Classic plan is suitable.\n",
      "This plan offers coverage of up to 1 crore with optional maternity benefits.\n",
      "You can opt for the maternity benefit which covers expenses for delivery and lawful termination up to 10% of the Annual Sum Insured, with a maximum limit of INR 1 Lakh.\n",
      "The waiting period for maternity benefit is 24 months from when the cover is opted.\n",
      "Maternity benefits are available under a family floater policy for members between 18 to 50 years.\n",
      "You can avail the new born baby cover if you opt for the maternity cover, providing additional financial protection.\n",
      "If you have already taken a policy without maternity benefit, you can add it only at the time of renewal.\n",
      "Remember that maternity benefits are not available outside the geographical boundaries of India.\n",
      "User:-  sure, i would like to buy Max Protect Classic\n",
      "Assistant:-  For your choice of Max Protect Classic, you will receive coverage of up to 1 crore.\n",
      "This plan offers comprehensive benefits with a high sum insured to meet your healthcare needs.\n",
      "If you have any specific questions or need further assistance, feel free to ask.\n",
      "User:-  Thank you\n",
      "Assistant:-  You're welcome! If you have any more questions or need further assistance, feel free to ask.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(temperature=1,model=\"gpt-3.5-turbo\")\n",
    "### Setting up Prompt\n",
    "prompt_template = \"\"\"You are a professional insurance advisor specializing in Elevate Policy Wordings, Health AdvantEdge_Policy wordings_IL, and Maxprotect. \n",
    "You must use the provided context and chat history to answer the user's questions accurately and comprehensively.\n",
    "If the user's query lacks sufficient information, ask clarifying questions to obtain these details. \n",
    "Be proactive in gathering information to provide personalized advice.\n",
    "Explain policy features in the context of the user's needs. For example, if discussing maternity coverage, explain how it would benefit a user planning to start a family.\n",
    "here you can find the major diffrance in all insurance offered {comparision}\n",
    "\n",
    "If the user asks about pricing, refer them to Shaurya Vashisth.\n",
    "\n",
    "The communication can be in english, hindi or hinglish.\n",
    "Give answers in small sentenses.\n",
    "\n",
    "User History: {user_history}\n",
    "Assistant History: {assistant_history}\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"comparision\",\"user_history\",\"assistant_history\",\"context\", \"question\"]\n",
    ")\n",
    "### Setting up RetrievalQA to retive similar chuncks\n",
    "### K = number of top k relevent chuncks\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "\n",
    "chat_logs = {\"User\":\"\",\"Assistant\":\"\"}\n",
    "\n",
    "while True:\n",
    "    Query = input(\"User:- \")\n",
    "    if Query.lower() == \"quit\":\n",
    "        break\n",
    "    else:\n",
    "        docs = retriever.invoke(Query)\n",
    "        full_prompt = PROMPT.format(user_history=chat_logs['User'],\n",
    "                assistant_history=chat_logs['Assistant'],\n",
    "                context=\"\\n\\n\".join(doc.page_content for doc in docs),\n",
    "                question=Query,\n",
    "                comparision = Comparison)\n",
    "        response = llm.invoke(full_prompt)\n",
    "        print(\"User:- \",Query)\n",
    "        print(\"Assistant:- \",response.content)\n",
    "        chat_logs['User'] = Query\n",
    "        chat_logs['Assistant'] = response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### build a logic to summarize the chats older that 5 iter\n",
    "#### Tune on temprature, top-p and top-k\n",
    "\n",
    "\n",
    "# The final step is to generate the next token by sampling from this distribution \n",
    "# The temperature hyperparameter plays a critical role in this process. Mathematically speaking,\n",
    "\n",
    "# it is a very simple operation: model output logits are simply divided by the temperature:\n",
    "# temperature = 1: Dividing logits by one has no effect on the softmax outputs.\n",
    "# temperature < 1: Lower temperature makes the model more confident and deterministic by sharpening the probability distribution, leading to more predictable outputs.\n",
    "# temperature > 1: Higher temperature creates a softer probability distribution, allowing for more randomness in the generated text – what some refer to as model “creativity”.\n",
    "\n",
    "# In addition, the sampling process can be further refined using top-k and top-p parameters:\n",
    "# top-k sampling: Limits the candidate tokens to the top k tokens with the highest probabilities, filtering out less likely options.\n",
    "# top-p sampling: Considers the smallest set of tokens whose cumulative probability exceeds a threshold p, ensuring that only the most likely tokens contribute while still allowing for diversity.\n",
    "# By tuning temperature, top-k, and top-p, you can balance between deterministic and diverse outputs, tailoring the model's behavior to your specific needs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
